{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "classes = tuple(x for x in \"_\" + string.ascii_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.EMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform, split=\"letters\")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.EMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform, split=\"letters\")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X    , tensor([24, 11,  1, 16])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.rot90(npimg,3)\n",
    "    npimg = np.fliplr(npimg)\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(images[0][0])\n",
    "# print labels\n",
    "print(f'{classes[labels[0]]:5s}, {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 24,  7,  3])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 120]          30,840\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 27]           2,295\n",
      "================================================================\n",
      "Total params: 45,871\n",
      "Trainable params: 45,871\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary \n",
    "\n",
    "torchsummary.summary(net, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.277\n",
      "[1,  4000] loss: 0.617\n",
      "[1,  6000] loss: 0.515\n",
      "[1,  8000] loss: 0.454\n",
      "[1, 10000] loss: 0.415\n",
      "[1, 12000] loss: 0.383\n",
      "[1, 14000] loss: 0.376\n",
      "[1, 16000] loss: 0.341\n",
      "[1, 18000] loss: 0.349\n",
      "[1, 20000] loss: 0.324\n",
      "[1, 22000] loss: 0.354\n",
      "[1, 24000] loss: 0.322\n",
      "[1, 26000] loss: 0.311\n",
      "[1, 28000] loss: 0.305\n",
      "[1, 30000] loss: 0.295\n",
      "[2,  2000] loss: 0.276\n",
      "[2,  4000] loss: 0.269\n",
      "[2,  6000] loss: 0.277\n",
      "[2,  8000] loss: 0.273\n",
      "[2, 10000] loss: 0.292\n",
      "[2, 12000] loss: 0.279\n",
      "[2, 14000] loss: 0.268\n",
      "[2, 16000] loss: 0.271\n",
      "[2, 18000] loss: 0.277\n",
      "[2, 20000] loss: 0.278\n",
      "[2, 22000] loss: 0.287\n",
      "[2, 24000] loss: 0.253\n",
      "[2, 26000] loss: 0.263\n",
      "[2, 28000] loss: 0.260\n",
      "[2, 30000] loss: 0.265\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  A     A     A     A    \n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "#imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  U     A     A     A    \n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 90 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: _     is 0.0 %\n",
      "Accuracy for class: A     is 89.8 %\n",
      "Accuracy for class: B     is 96.0 %\n",
      "Accuracy for class: C     is 97.2 %\n",
      "Accuracy for class: D     is 93.1 %\n",
      "Accuracy for class: E     is 91.8 %\n",
      "Accuracy for class: F     is 94.8 %\n",
      "Accuracy for class: G     is 73.0 %\n",
      "Accuracy for class: H     is 90.8 %\n",
      "Accuracy for class: I     is 46.2 %\n",
      "Accuracy for class: J     is 90.4 %\n",
      "Accuracy for class: K     is 94.6 %\n",
      "Accuracy for class: L     is 86.2 %\n",
      "Accuracy for class: M     is 98.8 %\n",
      "Accuracy for class: N     is 91.1 %\n",
      "Accuracy for class: O     is 96.9 %\n",
      "Accuracy for class: P     is 97.0 %\n",
      "Accuracy for class: Q     is 85.1 %\n",
      "Accuracy for class: R     is 93.0 %\n",
      "Accuracy for class: S     is 96.0 %\n",
      "Accuracy for class: T     is 94.1 %\n",
      "Accuracy for class: U     is 87.0 %\n",
      "Accuracy for class: V     is 90.1 %\n",
      "Accuracy for class: W     is 95.9 %\n",
      "Accuracy for class: X     is 94.5 %\n",
      "Accuracy for class: Y     is 96.2 %\n",
      "Accuracy for class: Z     is 96.8 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / (total_pred[classname]+ 0.00000000000001)\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPPA\n",
      "GORMUR\n",
      "HELLU\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "L = ['bigM', 'bigA','bigP','bigP','bigA']\n",
    "M = ['bigG', 'bigO','bigR','bigM','bigU','bigR']\n",
    "N = ['bigH', 'bigE','bigL','bigL','bigU']\n",
    "\n",
    "def read_from_image(L):\n",
    "    word = \"\"\n",
    "    for i in range(len(L)):\n",
    "        image = Image.open(\"examples/demo/\"+str(L[i])+\".png\")\n",
    "        image = image.convert(\"1\")\n",
    "        x = TF.to_tensor(image).squeeze()\n",
    "        x = x.numpy()\n",
    "        #print(x.shape)\n",
    "        x = np.fliplr(x)\n",
    "        x = np.rot90(x)\n",
    "        x = torch.from_numpy(x.copy())\n",
    "        #imshow(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        #print(x.shape)\n",
    "        output = net(x.unsqueeze(0))\n",
    "        output = torch.softmax(output, 1)\n",
    "        #print(output)\n",
    "        output = torch.argmax(output)\n",
    "        #print(f'{classes[output]:5s}') \n",
    "        word+=classes[output]\n",
    "    return word\n",
    "\n",
    "\n",
    "wordL = read_from_image(L)\n",
    "wordM = read_from_image(M)\n",
    "wordN = read_from_image(N)\n",
    "print(wordL)\n",
    "print(wordM)\n",
    "print(wordN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rétt\n",
      "Rétt\n",
      "Rétt\n"
     ]
    }
   ],
   "source": [
    "from islenska import Bin\n",
    "\n",
    "def er_þetta_orð(wordL):\n",
    "    b = Bin()\n",
    "    x = len(b.lookup(wordL)[1])\n",
    "    if(x==0): return \"Rangt, prófa aftur\"\n",
    "    else: return \"Rétt\"\n",
    "\n",
    "print(er_þetta_orð(wordL))\n",
    "print(er_þetta_orð(wordM))\n",
    "print(er_þetta_orð(wordN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
